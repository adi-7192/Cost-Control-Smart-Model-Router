# ğŸš€ Cost-Control Smart Model Router

An intelligent LLM routing system that automatically selects the most cost-effective AI model based on prompt complexity, reducing API costs by up to 70% while maintaining response quality.

## âœ¨ Features

- **ğŸ§  Smart Routing**: Automatically classifies prompts and routes to optimal models
- **ğŸ’° Cost Optimization**: Uses cheaper models for simple tasks, expensive ones only when needed
- **ï¿½ Real-time Savings Comparison**: See exactly how much you save vs using GPT-4o for everything
- **ğŸ“ˆ Scale Projections**: Visualize daily, monthly, and yearly savings at 1000 requests/day
- **ï¿½ğŸ”Œ Multi-Provider Support**: Works with OpenAI (GPT-4o), Google (Gemini), and more
- **ğŸ“Š Real-time Analytics**: Track costs, usage, and cumulative savings via Streamlit dashboard
- **ğŸ¯ Intelligent Classification**: LLM-powered or rule-based prompt analysis
- **ğŸ”„ Auto-Discovery**: Automatically detects and uses best available models
- **ğŸ¨ Modern UI**: Beautiful Streamlit dashboard with dark theme and gradient cards
- **ğŸ› ï¸ Extensible**: Easy to add new models and classifiers

## ğŸ¯ How It Works

```
User Prompt â†’ Smart Classifier â†’ Route to Best Model â†’ Return Response
                    â†“
            [Simple] â†’ Phi-3 (Fast & Cheap)
            [Moderate] â†’ Gemini 2.5 Flash (Balanced)
            [Complex] â†’ GPT-4o (Powerful)
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10-3.13 (recommended: 3.13)
- pip
- (Optional) OpenAI API Key
- (Optional) Google AI API Key

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/cost-control-smart-router.git
   cd cost-control-smart-router
   ```

2. **Create virtual environment**
   ```bash
   python3.13 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env and add your API keys (optional for demo)
   ```

5. **Run the backend server**
   ```bash
   uvicorn app.main:app --reload
   ```

6. **Run the Streamlit dashboard** (in a new terminal)
   ```bash
   streamlit run dashboard.py
   ```

7. **Access the application**
   - Dashboard: http://localhost:8501
   - API: http://localhost:8000

## ğŸ”‘ Configuration

### Environment Variables

Create a `.env` file (or copy from `.env.example`):

```bash
# Classifier Type
CLASSIFIER_TYPE=rules  # Options: "rules" or "llm"

# Database
DATABASE_URL=sqlite:///./sql_app.db

# API Keys (Optional - system works without them in demo mode)
OPENAI_API_KEY=your-openai-key-here
GOOGLE_API_KEY=your-google-api-key-here
```

### Adding API Keys via Dashboard

1. Open the Streamlit dashboard at http://localhost:8501
2. Click **Settings** in the sidebar
3. Expand **ğŸ”‘ API Configuration**
4. Enter your API keys
5. Click **ğŸ’¾ Save API Keys**

The system will automatically:
- Detect available models
- Enable smart LLM-based classification
- Route to real AI models

## ğŸ“Š API Endpoints

### `POST /route`
Route a prompt to the best model

**Request:**
```json
{
  "prompt": "Explain quantum physics"
}
```

**Response:**
```json
{
  "model": "GPT-4o",
  "difficulty": "complex",
  "reasoning": "Requires deep technical explanation",
  "response": "Quantum physics is...",
  "cost": 0.0075,
  "tokens": 250,
  "latency_ms": 1200
}
```

### `GET /stats`
Get usage statistics

### `GET /logs`
Get recent request history (last 100)

### `POST /config/keys`
Update API keys programmatically

## ğŸ¨ Dashboard Features

- **ğŸ’¬ Prompt Testing**: Submit prompts and see real-time routing decisions
- **ğŸ“ Response Display**: View full AI-generated responses
- **ï¿½ Cost Savings Comparison**: Beautiful gradient card showing:
  - What you paid (with smart routing)
  - What GPT-4o would have cost
  - Exact savings amount and percentage
- **ï¿½ğŸ“Š Scale Projections**: See potential savings at 1000 requests/day:
  - Daily, monthly, and yearly projections
  - Big picture impact visualization
- **ğŸ“Š Metadata**: See model used, cost, latency, and reasoning
- **ğŸ“ˆ Analytics Dashboard**: 
  - Total requests and actual costs
  - Cost without routing comparison
  - **Cumulative savings tracker**
  - Model and difficulty distribution charts
- **ğŸ“‹ Recent Logs**: Monitor all routing decisions in a table
- **âš™ï¸ Settings**: Add/update API keys directly from sidebar
- **ğŸ¨ Modern UI**: Dark theme with horizontal charts and clean layout

## ğŸ§© Extending the System

### Adding a New Model

1. Create a new client in `app/llm/providers.py`:

```python
class ClaudeClient(LLMClient):
    async def generate(self, prompt: str, max_tokens: int = 100):
        # Your implementation
        return response_text, cost, tokens
```

2. Register in router (`app/router.py`):

```python
self.clients = {
    "simple": Phi3Client(),
    "moderate": GeminiClient(),
    "complex": ClaudeClient()  # New model
}
```

See `EXTENDING.md` for detailed instructions.

## ğŸ“ˆ Cost Savings Example

**Without Smart Routing** (using GPT-4o for everything):
- 1000 requests/day
- Average: $0.03/request
- **Monthly cost: $900**

**With Smart Routing**:
- 600 simple â†’ Phi-3 ($0.00005/request) = $0.03
- 300 moderate â†’ Gemini ($0.0005/request) = $0.15
- 100 complex â†’ GPT-4o ($0.03/request) = $3.00
- **Monthly cost: $3.18** (99.6% savings!)

## ğŸ› ï¸ Development

### Running Tests

```bash
python verify.py
```

### Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ router.py            # Core routing logic
â”‚   â”œâ”€â”€ models.py            # Pydantic/SQLAlchemy models
â”‚   â”œâ”€â”€ database.py          # Database setup
â”‚   â”œâ”€â”€ config.py            # Configuration
â”‚   â”œâ”€â”€ classifier/          # Prompt classifiers
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ rules.py         # Rule-based classifier
â”‚   â”‚   â”œâ”€â”€ llm.py           # LLM-based classifier
â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â””â”€â”€ llm/                 # LLM clients
â”‚       â”œâ”€â”€ base.py
â”‚       â”œâ”€â”€ providers.py     # Model implementations
â”‚       â”œâ”€â”€ factory.py
â”‚       â””â”€â”€ model_discovery.py
â”œâ”€â”€ dashboard.py             # Streamlit dashboard
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/) and [Streamlit](https://streamlit.io/)
- Powered by OpenAI and Google AI
- Inspired by the need for cost-effective AI solutions

---

**â­ If you find this project useful, please consider giving it a star!**
